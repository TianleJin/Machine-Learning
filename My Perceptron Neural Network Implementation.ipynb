{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Neural Network\n",
    "In this notebook, we will look at the most fundamental type of neural network called Perceptron Neural Network. We will create a neural network for recognising handwritten digits using Python's Keras library and also implement our own neural network from scratch to perform the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data and taking a look its format \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise the data\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the images are 28 by 28 pixels of values ranging between 0 and 255\n",
    "# we first need to reshape the data so that it can be used as an input to our network\n",
    "pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape((X_train.shape[0], pixels))\n",
    "X_test = X_test.reshape((X_test.shape[0], pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will normalise the data \n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will convert y_train and y_test to categorical matrices\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the number of output categories\n",
    "n_categories = y_train.shape[1]\n",
    "n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a neural network using keras\n",
    "def digit_classifier():\n",
    "    model = Sequential()\n",
    "    # hidden layers\n",
    "    model.add(Dense(100, activation='relu', input_shape=(pixels,)))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # output layer\n",
    "    model.add(Dense(n_categories, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.2512 - accuracy: 0.9258 - val_loss: 0.1284 - val_accuracy: 0.9596\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.1069 - accuracy: 0.9682 - val_loss: 0.0940 - val_accuracy: 0.9710\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0744 - accuracy: 0.9765 - val_loss: 0.0851 - val_accuracy: 0.9743\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0559 - accuracy: 0.9822 - val_loss: 0.0887 - val_accuracy: 0.9748\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0446 - accuracy: 0.9855 - val_loss: 0.0778 - val_accuracy: 0.9771\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0368 - accuracy: 0.9878 - val_loss: 0.0841 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 0.0991 - val_accuracy: 0.9735\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 0.0831 - val_accuracy: 0.9776\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.1016 - val_accuracy: 0.9755\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 0.1041 - val_accuracy: 0.9754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x184b08271d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "clf = digit_classifier()\n",
    "clf.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 58us/step\n",
      "loss of network: 0.10414411052670912\n",
      "accuracy of network on test data: 0.9753999710083008\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of the model using testing data\n",
    "loss, accuracy = clf.evaluate(X_test, y_test)\n",
    "print('loss of network: {}'.format(loss))\n",
    "print('accuracy of network on test data: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My implementation of neural network\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        \"\"\"init method\n",
    "        :arg n: 1darray or list containing the number of nodes in each layer in the network\n",
    "        \"\"\"\n",
    "        # initialise network attributes\n",
    "        self.layers = len(n)\n",
    "        self.weights = [np.sqrt(2 / n[l - 1]) * np.random.randn(n[l], n[l - 1]) for l in range(1, len(n))]\n",
    "        self.biases = [0.01 * np.ones(n[l]) for l in range(1, len(n))]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"this method predicts the outputs of inputs X using forward propagation.\n",
    "        :arg X: ndarray of dimensions m by n where m is the number of data points and n is the number of features\n",
    "        :return outputs: ndarray of outputs from the network\n",
    "        \"\"\"\n",
    "        # initialise prediction matrix\n",
    "        m = X.shape[0]\n",
    "        n = self.biases[-1].shape[0]\n",
    "        outputs = np.zeros((m, n))\n",
    "        \n",
    "        for i in range(m):\n",
    "            inputs = X[i]\n",
    "            \n",
    "            # forward prop\n",
    "            for layer in range(self.layers - 1):\n",
    "                z = np.sum(self.weights[layer] * inputs, axis=1) + self.biases[layer]\n",
    "                a = self.relu(z) if layer != self.layers - 2 else self.sigmoid(z)\n",
    "                inputs = a\n",
    "            outputs[i] = inputs\n",
    "        return outputs\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"this method evaluates the accuracy of the outputs from the neural network using y as the ground truth.\n",
    "        :arg X: ndarray of training data\n",
    "        :arg y: ndarray of expected outputs\n",
    "        :return accuracy: the accuracy of the predictions made using the neural network\n",
    "        \"\"\"\n",
    "        y_hat = self.predict(X)\n",
    "        return np.sum(np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)) / len(y_hat)\n",
    "    \n",
    "    def train(self, X, y, batchsize=32, epochs=10, eta=0.01):\n",
    "        \"\"\"this method trains the neural network using the entire data set.\n",
    "        :arg X: ndarray of training data\n",
    "        :arg y: ndarray of expected outputs\n",
    "        :batchsize: (int) the number of data in each training batch\n",
    "        :epochs: (int) the number of iterations through the entire training data\n",
    "        :eta: (float) the learning rate of the network\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # initialise timer\n",
    "            print('epoch {}...'.format(epoch + 1))\n",
    "            start = default_timer()\n",
    "            \n",
    "            # train the data by batch\n",
    "            for i in range(0, len(X), batchsize):\n",
    "                self.train_mini_batch(X[i:i + batchsize], y[i:i + batchsize], eta)\n",
    "                \n",
    "            # calculate MSE to make sure that the network is training properly\n",
    "            MSE = np.mean((y - self.predict(X)) ** 2)\n",
    "            accuracy = self.evaluate(X, y)\n",
    "            print('epoch {} completed -- MSE: {} -- time taken to train: {}s -- accuracy: {}'.format(epoch + 1, MSE, default_timer() - start, accuracy))\n",
    "    \n",
    "    def train_mini_batch(self, X, y, eta):\n",
    "        \"\"\"this method trains the network using one batch of the dataset.\n",
    "        :arg X: ndarray of a batch of training data\n",
    "        :arg y: ndarray of the expected outputs from the training data\n",
    "        :arg eta: (float) learning rate of the network\n",
    "        \"\"\"\n",
    "        # initialise gradient parameters\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        # perform forward prop and backprop for each data point in the batch\n",
    "        for i in range(len(X)):\n",
    "            delta_nabla_w, delta_nabla_b = self.backprop(X[i], y[i])\n",
    "            nabla_w = [w + dw for w, dw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_b = [b + db for b, db in zip(nabla_b, delta_nabla_b)]\n",
    "        \n",
    "        # update weights and biases\n",
    "        self.weights = [w - (eta / len(X)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(X)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    def backprop(self, X, y):\n",
    "        \"\"\"this method performs one iteration of the backpropagation algorithm in the network\n",
    "        :arg X: ndarray of a batch of training data\n",
    "        :arg y: ndarray of the expected outputs from the training data\n",
    "        :return nabla_w: list of ndarrays of partial derivatives of cost with respect to weights\n",
    "        :return nabla_b: list of ndarrays of partial derivatives of cost with respect to biases\n",
    "        \"\"\"\n",
    "        # initialise gradient parameters\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        inputs = X\n",
    "        zs = []\n",
    "        activations = [X]\n",
    "        \n",
    "        # forwardprop\n",
    "        for layer in range(self.layers - 1):\n",
    "            # calculate weighted sums\n",
    "            z = np.sum(self.weights[layer] * inputs, axis=1) + self.biases[layer]\n",
    "            # calculate activations\n",
    "            a = self.relu(z) if layer != self.layers - 2 else self.sigmoid(z)\n",
    "            \n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "            inputs = a\n",
    "        \n",
    "        # backprop\n",
    "        delta = (activations[-1] - y) * self.sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta.reshape((len(delta), 1)), activations[-2].reshape(1, len(activations[-2])))\n",
    "        \n",
    "        # calculate delta for hidden layers\n",
    "        for l in range(2, self.layers):\n",
    "            z = zs[-l]\n",
    "            delta = np.sum(self.weights[-l + 1].T * delta, axis=1) * self.relu_prime(z)\n",
    "            \n",
    "            nabla_b[-l] = delta\n",
    "            a = activations[-l - 1]\n",
    "            nabla_w[-l] = np.dot(delta.reshape((len(delta), 1)), a.reshape(1, len(a)))\n",
    "        \n",
    "        return (nabla_w, nabla_b)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        \"\"\"this method acts as the relu activation function\n",
    "        :arg z: 1darray of weighted outputs from a layer\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_prime(self, z):\n",
    "        \"\"\"this method acts as the derivation of the relu activation function\n",
    "        :arg z: 1darray of weighted outputs from a layer\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"this method acts as the sigmoid activation function\n",
    "        :arg z: 1darray of weighted outputs from a layer\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        \"\"\"this method acts as the derivative of the sigmoid activation function\n",
    "        :arg z: 1darray of weighted outputs from a layer\n",
    "        \"\"\"\n",
    "        a = self.sigmoid(z)\n",
    "        return a * (1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1...\n",
      "epoch 1 completed -- MSE: 0.04392331027115827 -- time taken to train: 119.9255211s -- accuracy: 0.728\n",
      "epoch 2...\n",
      "epoch 2 completed -- MSE: 0.029436232735420204 -- time taken to train: 108.61293460000002s -- accuracy: 0.85425\n",
      "epoch 3...\n",
      "epoch 3 completed -- MSE: 0.023223545072979617 -- time taken to train: 114.14436300000003s -- accuracy: 0.8800166666666667\n",
      "epoch 4...\n",
      "epoch 4 completed -- MSE: 0.02017143428357226 -- time taken to train: 111.5192854s -- accuracy: 0.8936\n",
      "epoch 5...\n",
      "epoch 5 completed -- MSE: 0.018259871459302444 -- time taken to train: 114.18007409999996s -- accuracy: 0.9019666666666667\n",
      "epoch 6...\n",
      "epoch 6 completed -- MSE: 0.016883735147000464 -- time taken to train: 107.52729529999999s -- accuracy: 0.9081166666666667\n",
      "epoch 7...\n",
      "epoch 7 completed -- MSE: 0.015807864551204604 -- time taken to train: 124.78141189999997s -- accuracy: 0.9132333333333333\n",
      "epoch 8...\n",
      "epoch 8 completed -- MSE: 0.014936764777263306 -- time taken to train: 124.58522419999997s -- accuracy: 0.9170666666666667\n",
      "epoch 9...\n",
      "epoch 9 completed -- MSE: 0.014206502154588322 -- time taken to train: 121.81798619999995s -- accuracy: 0.9209666666666667\n",
      "epoch 10...\n",
      "epoch 10 completed -- MSE: 0.013577158101703957 -- time taken to train: 61.11675910000008s -- accuracy: 0.9242666666666667\n",
      "epoch 11...\n",
      "epoch 11 completed -- MSE: 0.013025905244955211 -- time taken to train: 56.9203265000001s -- accuracy: 0.9271166666666667\n",
      "epoch 12...\n",
      "epoch 12 completed -- MSE: 0.012536371318526075 -- time taken to train: 65.82731260000014s -- accuracy: 0.9301666666666667\n",
      "epoch 13...\n",
      "epoch 13 completed -- MSE: 0.012097024709214244 -- time taken to train: 68.08656930000006s -- accuracy: 0.9324666666666667\n",
      "epoch 14...\n",
      "epoch 14 completed -- MSE: 0.011699731378878572 -- time taken to train: 57.49272629999996s -- accuracy: 0.9344666666666667\n",
      "epoch 15...\n",
      "epoch 15 completed -- MSE: 0.011339470546281049 -- time taken to train: 67.29827010000008s -- accuracy: 0.9365333333333333\n",
      "epoch 16...\n",
      "epoch 16 completed -- MSE: 0.011009715374364784 -- time taken to train: 55.79167000000007s -- accuracy: 0.93845\n",
      "epoch 17...\n",
      "epoch 17 completed -- MSE: 0.010705934809389195 -- time taken to train: 56.97113250000007s -- accuracy: 0.94005\n",
      "epoch 18...\n",
      "epoch 18 completed -- MSE: 0.010424203693963768 -- time taken to train: 60.907302900000104s -- accuracy: 0.9413333333333334\n",
      "epoch 19...\n",
      "epoch 19 completed -- MSE: 0.01016432032989223 -- time taken to train: 58.78083730000003s -- accuracy: 0.9426833333333333\n",
      "epoch 20...\n",
      "epoch 20 completed -- MSE: 0.009921389582232681 -- time taken to train: 55.53000730000008s -- accuracy: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# training our neural network\n",
    "n = [pixels, 100, 100, n_categories]\n",
    "network = NeuralNetwork(n)\n",
    "network.train(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the networks prediction of test data: 0.9441\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of the network''s prediction of test data: {}'.format(network.evaluate(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
